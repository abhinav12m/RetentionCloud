# -*- coding: utf-8 -*-
"""CLV&CHURN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HgCfObegMJyE1sAUcjY_LDsW3R5Sz9OO
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from xgboost import XGBRegressor, XGBClassifier
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, f1_score, roc_auc_score, classification_report
from imblearn.over_sampling import SMOTE
import shap
import joblib

from google.colab import drive
drive.mount('/content/drive')

# Load dataset
file_path = '/content/drive/MyDrive/Datasets/Churn_Modelling.csv'
data = pd.read_csv(file_path)

data

# Preprocessing and Feature Engineering
data = data.drop_duplicates()
data['Gender'] = data['Gender'].map({'Female': 0, 'Male': 1})
data = pd.get_dummies(data, columns=["Geography"], drop_first=True)
data.drop(columns=["RowNumber", "CustomerId", "Surname"], inplace=True)
data.fillna(0, inplace=True)

# Feature Engineering
data["Revenue_Proxy"] = data["Balance"] * data["NumOfProducts"]
data["Engagement_Score"] = data["IsActiveMember"] + data["HasCrCard"] + data["NumOfProducts"]
data["Risk_Indicator"] = data["CreditScore"] / data["Age"]
data["Spending_Potential"] = data["EstimatedSalary"] * data["NumOfProducts"]
data["Tenure_Weighted_Balance"] = data["Balance"] * data["Tenure"]
data["HVC"] = (data["Balance"] > data["Balance"].median()).astype(int)

data

# Drop rows with any infinite values
data = data[~np.isinf(data).any(axis=1)]

# CLV Prediction Features and Target
X_clv = data[["CreditScore", "Age", "Tenure", "Balance", "NumOfProducts", "HasCrCard",
              "IsActiveMember", "EstimatedSalary", "Geography_Germany", "Geography_Spain",
              "Engagement_Score", "Risk_Indicator", "Spending_Potential", "Tenure_Weighted_Balance", "HVC"]]
y_clv = data["Revenue_Proxy"]

# Churn Prediction Features and Target
X_churn = X_clv.copy()
y_churn = data["Exited"]

# Train-Test Split for CLV and Churn Prediction
X_train_clv, X_test_clv, y_train_clv, y_test_clv = train_test_split(X_clv, y_clv, test_size=0.25, random_state=42)
X_train_churn, X_test_churn, y_train_churn, y_test_churn = train_test_split(X_churn, y_churn, test_size=0.25, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train_clv_scaled = scaler.fit_transform(X_train_clv)
X_test_clv_scaled = scaler.transform(X_test_clv)
X_train_churn_scaled = scaler.fit_transform(X_train_churn)
X_test_churn_scaled = scaler.transform(X_test_churn)

# Handle Class Imbalance for Churn Prediction using SMOTE
smote = SMOTE(random_state=42)
X_train_churn_balanced, y_train_churn_balanced = smote.fit_resample(X_train_churn_scaled, y_train_churn)

# CLV Model Training and Evaluation
clv_models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42)
}

rmse_scores = []
r2_scores = []

for name, model in clv_models.items():
    model.fit(X_train_clv_scaled, y_train_clv)
    y_pred = model.predict(X_test_clv_scaled)
    rmse_scores.append(np.sqrt(mean_squared_error(y_test_clv, y_pred)))
    r2_scores.append(r2_score(y_test_clv, y_pred))

# Visualization for CLV Model Performance
models_list = list(clv_models.keys())
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.barplot(x=models_list, y=rmse_scores, palette='viridis')
plt.title('CLV Model Performance (RMSE)')
plt.ylabel('RMSE')
plt.xlabel('Models')

plt.subplot(1, 2, 2)
sns.barplot(x=models_list, y=r2_scores, palette='viridis')
plt.title('CLV Model Performance (R² Score)')
plt.ylabel('R² Score')
plt.xlabel('Models')

plt.tight_layout()
plt.show()

# Get feature importance
xgb_model_clv = XGBRegressor(random_state=42)
xgb_model_clv.fit(X_train_clv_scaled, y_train_clv)
feature_importance_clv = xgb_model_clv.feature_importances_

# Visualize feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_clv, y=X_clv.columns, palette='viridis')
plt.title('XGBoost Feature Importance for CLV Prediction')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()

# Churn Model Training and Evaluation
churn_models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(random_state=42)
}

accuracy_scores = []
f1_scores = []

for name, model in churn_models.items():
    model.fit(X_train_churn_balanced, y_train_churn_balanced)
    y_pred = model.predict(X_test_churn_scaled)
    accuracy_scores.append(accuracy_score(y_test_churn, y_pred))
    f1_scores.append(f1_score(y_test_churn, y_pred))

# Visualization for Churn Model Performance
models_list_churn = list(churn_models.keys())
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.barplot(x=models_list_churn, y=accuracy_scores, palette='coolwarm')
plt.title('Churn Model Performance (Accuracy)')
plt.ylabel('Accuracy')
plt.xlabel('Models')

plt.subplot(1, 2, 2)
sns.barplot(x=models_list_churn, y=f1_scores, palette='coolwarm')
plt.title('Churn Model Performance (F1 Score)')
plt.ylabel('F1 Score')
plt.xlabel('Models')

plt.tight_layout()
plt.show()

# Feature Importance for Churn (Using Random Forest as an example)
rf_model_churn = RandomForestClassifier(random_state=42)
rf_model_churn.fit(X_train_churn_balanced, y_train_churn_balanced)
feature_importance_churn = rf_model_churn.feature_importances_

plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_churn, y=X_churn.columns, palette='coolwarm')
plt.title('Feature Importance for Churn Prediction')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.show()

# Determine best models
best_clv_model_name = models_list[np.argmin(rmse_scores)]
best_churn_model_name = models_list_churn[np.argmax(f1_scores)]

best_clv_model = clv_models[best_clv_model_name]
best_churn_model = churn_models[best_churn_model_name]


print(f"Best clv model {best_clv_model_name}")
print(f"Best churn model {best_churn_model_name}")

import joblib
import tarfile
import os

# Define save directory
save_dir = '/content/drive/MyDrive/Datasets/'

# Save the best CLV model
clv_model_path = os.path.join(save_dir, "clv_model.pkl")
joblib.dump(best_clv_model, clv_model_path)

clv_tar_path = os.path.join(save_dir, "clv_model.tar.gz")
with tarfile.open(clv_tar_path, "w:gz") as tar:
    tar.add(clv_model_path, arcname="clv_model.pkl")

# Save the best Churn model
churn_model_path = os.path.join(save_dir, "churn_model.pkl")
joblib.dump(best_churn_model, churn_model_path)

churn_tar_path = os.path.join(save_dir, "churn_model.tar.gz")
with tarfile.open(churn_tar_path, "w:gz") as tar:
    tar.add(churn_model_path, arcname="churn_model.pkl")

print(f"Models saved at:\n{clv_tar_path}\n{churn_tar_path}")